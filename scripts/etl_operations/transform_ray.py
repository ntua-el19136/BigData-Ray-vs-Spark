import ray
from pyarrow import fs
import sys
import time
import pandas as pd

ray.init()

start_time = time.time()

hdfs_fs = fs.HadoopFileSystem.from_uri("hdfs://okeanos-master:54310")

input_path = sys.argv[1]

ds = ray.data.read_csv(input_path, filesystem=hdfs_fs)
print("âœ… Dataset loaded")

ds = ds.repartition(200)

def add_new_feature(batch: pd.DataFrame) -> pd.DataFrame:
    batch["new_feature"] = (batch["feature_1"] ** 2 + batch["feature_2"] ** 2).pow(0.5)
    return batch

ds = ds.map_batches(add_new_feature, batch_format="pandas", batch_size=64000)
print("â• Î¥Ï€Î¿Î»Î¿Î³Î¯ÏƒÏ„Î·ÎºÎµ Î· ÏƒÏ„Î®Î»Î· 'new_feature'.")

def vectorized_filter(batch: pd.DataFrame) -> pd.DataFrame:
    mask = batch["word"].str.len() > batch["new_feature"]
    return batch[mask]

ds = ds.map_batches(vectorized_filter, batch_format="pandas", batch_size=64000)
print("ğŸ” Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Î·ÎºÎµ Ï†Î¯Î»Ï„ÏÎ¿ length(word) > new_feature.")

ds = ds.materialize()
print("ğŸ“Š Î”ÎµÎ¯Î³Î¼Î±:")
ds.show(5)
print("ğŸ“ Î£ÏÎ½Î¿Î»Î¿ Î³ÏÎ±Î¼Î¼ÏÎ½:", ds.count())

end_time = time.time()
print(f"â±ï¸ Î£Ï…Î½Î¿Î»Î¹ÎºÏŒÏ‚ Ï‡ÏÏŒÎ½Î¿Ï‚ ÎµÎºÏ„Î­Î»ÎµÏƒÎ·Ï‚: {end_time - start_time:.2f} Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î±")

print(ds.stats())
